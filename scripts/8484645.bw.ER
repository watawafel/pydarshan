-------------------------------------------------------------------
/mnt/a/u/eot/borcean2/apps/spark/2.1.1/modulefiles/spark-2.1.1:

module-whatis	 Spark implementation for Blue Waters via Shifter.  
setenv		 SPARK_MASTER_HOST nid22142 
setenv		 SPARK_HOME /usr/local/bin/spark-2.1.1-bin-hadoop2.7 
setenv		 SPARK_SCRIPTS /mnt/a/u/eot/borcean2/apps/spark/2.1.1/ 
setenv		 SPARK_CONF_DIR /mnt/a/u/eot/borcean2/apps/spark/2.1.1//conf 
setenv		 CRAY_ROOTFS SHIFTER 
prepend-path	 PATH /mnt/a/u/eot/borcean2/apps/spark/2.1.1//bin 
-------------------------------------------------------------------

-------------------------------------------------------------------
/sw/bw/modulefiles/bwpy/1.1.0:

conflict	 boost 
conflict	 visit 
conflict	 vtk 
conflict	 bwpy 
conflict	 cmake 
conflict	 bwpy-mpi/0.3.0 
setenv		 BWPY_VERSION 1.1.0 
setenv		 BWPY_DIR /mnt/bwpy/single 
prepend-path	 PATH /opt/bwpy/bin 
prepend-path	 PATH /sw/bw/bwpy/mnt/bin 
setenv		 BWPY_INCLUDE_PATH /mnt/bwpy/single/include 
setenv		 BWPY_LIBRARY_PATH /mnt/bwpy/single/lib:/mnt/bwpy/single/usr/lib 
setenv		 BWPY_MANPATH /mnt/bwpy/single/usr/share/man 
prepend-path	 MANPATH /mnt/bwpy/single/usr/share/man 
setenv		 BWPY_PKG_PATH /mnt/bwpy/single/usr/lib/pkgconfig 
prepend-path	 PKG_CONFIG_PATH /mnt/bwpy/single/usr/lib/pkgconfig 
setenv		 BWPY_BINARY_PATH /mnt/bwpy/single/bin:/mnt/bwpy/single/usr/bin 
prepend-path	 PATH /mnt/bwpy/single/bin:/mnt/bwpy/single/usr/bin 
setenv		 BWPY_MODULEPATH /mnt/bwpy/single/usr/share/modulefiles 
prepend-path	 MODULEPATH /mnt/bwpy/single/usr/share/modulefiles 
-------------------------------------------------------------------

[I 20:07:48.063 NotebookApp] Serving notebooks from local directory: /mnt/a/u/eot/borcean2
[I 20:07:48.063 NotebookApp] 0 active kernels 
[I 20:07:48.063 NotebookApp] The Jupyter Notebook is running at: http://10.128.87.16:8890/
[I 20:07:48.063 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 20:10:11.423 NotebookApp] 302 GET / (10.0.0.147) 0.87ms
[I 20:10:11.434 NotebookApp] 302 GET /tree? (10.0.0.147) 1.56ms
[I 20:10:13.468 NotebookApp] 302 POST /login?next=%2Ftree%3F (10.0.0.147) 1.76ms
[W 20:10:15.665 NotebookApp] 404 GET /favicon.ico (10.0.0.147) 15.57ms referer=None
[W 20:10:15.668 NotebookApp] 404 GET /favicon.ico (10.0.0.147) 2.21ms referer=None
[W 20:10:20.147 NotebookApp] Notebook apps/pydarshan/notebooks/mine.ipynb is not trusted
[I 20:10:21.260 NotebookApp] Kernel started: 7b3fc7fd-3d45-4421-839d-4c28eadaf355
[IPKernelApp] WARNING | File not found: '/etc/pythonstart'
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
18/04/05 20:11:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[Stage 0:>                                                         (0 + 0) / 64][Stage 0:>                                                        (0 + 64) / 64]18/04/05 20:11:18 WARN TaskSetManager: Lost task 53.0 in stage 0.0 (TID 53, 10.128.87.46, executor 0): org.apache.spark.SparkException: 
Error from python worker:
  Error: Not a root suid binary! (When launched via aprun, `aprun -b` must be used)
PYTHONPATH was:
  /usr/local/bin/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip:/usr/local/bin/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/usr/local/bin/spark-2.1.1-bin-hadoop2.7/jars/spark-core_2.11-2.1.1.jar:/usr/local/bin/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/usr/local/bin/spark-2.1.1-bin-hadoop2.7/python:/python/:/opt/xalt/0.7.6/sles11.3/libexec:/opt/cray/sdb/1.1-1.0502.63652.4.27.gem/lib64/py
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:166)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:116)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

[Stage 0:>                                                        (0 + 63) / 64][Stage 0:>                                                        (0 + 64) / 64][Stage 0:>                                                        (0 + 63) / 64][Stage 0:>                                                        (0 + 64) / 64]18/04/05 20:11:20 ERROR TaskSetManager: Task 49 in stage 0.0 failed 4 times; aborting job
[I 20:12:21.280 NotebookApp] Saving file at /apps/pydarshan/notebooks/mine.ipynb
[W 20:12:21.282 NotebookApp] Saving untrusted notebook apps/pydarshan/notebooks/mine.ipynb
18/04/05 20:14:07 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 248, 10.128.87.15, executor 1): org.apache.spark.SparkException: 
Error from python worker:
  Error: Not a root suid binary! (When launched via aprun, `aprun -b` must be used)
PYTHONPATH was:
  /usr/local/bin/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip:/usr/local/bin/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/usr/local/bin/spark-2.1.1-bin-hadoop2.7/jars/spark-core_2.11-2.1.1.jar:/usr/local/bin/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/usr/local/bin/spark-2.1.1-bin-hadoop2.7/python:/python/:/opt/xalt/0.7.6/sles11.3/libexec:/opt/cray/sdb/1.1-1.0502.63652.4.27.gem/lib64/py
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:166)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:116)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

[Stage 1:>                                                        (0 + 64) / 64]18/04/05 20:14:08 ERROR TaskSetManager: Task 58 in stage 1.0 failed 4 times; aborting job
