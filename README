This is a Spark / pyDarshan implementation for Blue Waters

Steps:

  1. Follow Spark Setup pdf
    -https://github.com/watawafel/pydarshan/blob/master/BWDOC-SparkClustersetup-150218-1307-4152%20(1).pdf
    
  2. Download pyDarshan
    -https://xgitlab.cels.anl.gov/darshan/darshan/tree/autoperf-mod/darshan-util
    
  3. Submit logs.pbs job script
    -https://github.com/watawafel/pydarshan/blob/master/scripts/logs.pbs


When submitting job script - Spark will allocate two nodes from resource. One for scheduler node and one for jupyter.

JOBID.ER / JOBID.OU will be created. "cat" files and identify ip for local jupyter compute node. 

To run Jupyter Notebook - Port forward compute node to localhost: ssh -L 9999:<JUPYTERIP>:8890 USERID@<LOGINNODE>.ncsa.illinois.edu


Lessons Learned:

  1. Mounting volumes onto containers and linking library binaries. 
    - Python libraries
    - Image mount
  2. Port forwarding on HPC system is slightly tedious. 
    - Need to connect to compute node running Jupyter to open. 
  3. Versions and dependencies, of everything, is a blast to deal with. 
    - Python path, version and linking dependencies.
  4. Environment variables - very important. 
  
 
The conclusion of this project came to a dead end with different versions of Spark Anaconda being used within the container and within bwpy (Blue Waters python) library. So, blow old image away. Start from scratch. 

Solution:

  Pull Spark NERSC lgerhardt image implemented on Shifter, update everything.


